{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10acd54",
   "metadata": {},
   "source": [
    "# Prediksi Tingkat Kemiskinan di Indonesia\n",
    "\n",
    "**Deskripsi singkat:** Notebook ini berisi langkah-langkah lengkap (step-by-step) untuk melakukan *data mining* dan membangun model prediksi tingkat kemiskinan di Indonesia dengan menggabungkan tiga dataset Kaggle yang Anda berikan.\n",
    "\n",
    "**Dataset yang dipakai (letakkan file CSV di folder `data/`)**:\n",
    "- `data/klasifikasi_kemiskinan.csv`  (dataset target)\n",
    "- `data/socio_economic_2021.csv`     (fitur sosial-ekonomi)\n",
    "- `data/pendidikan_provinsi_2023.csv` (indikator pendidikan per provinsi)\n",
    "\n",
    "> Jika nama file CSV Anda berbeda, ubah path di cell `FILE PATHS` di bawah.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c11d82",
   "metadata": {},
   "source": [
    "## 0. Persiapan & Install (opsional)\n",
    "\n",
    "Beberapa package mungkin perlu di-install. Jalankan cell ini (hapus komentar `!`) jika paket belum tersedia di environment Anda. Instalasi `geopandas` dan `folium` kadang memerlukan dependensi sistem — jika instalasi gagal, Anda bisa skip bagian peta dan jalankan analisis model saja.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d33c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm shap folium geopandas plotly category_encoders imbalanced-learn joblib\n",
    "# Jika instalasi gagal pada beberapa package (mis. geopandas), lewati peta dan jalankan model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ae720",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98de878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Optional / advanced\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except Exception as e:\n",
    "    xgb = None\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "except Exception:\n",
    "    shap = None\n",
    "\n",
    "import joblib\n",
    "\n",
    "print('libraries loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48fbc7f",
   "metadata": {},
   "source": [
    "## 2. FILE PATHS\n",
    "\n",
    "Pastikan CSV Anda sudah di-download dari Kaggle dan diletakkan di folder `data/`. Jika tidak, ubah path di bawah sesuai lokasi file Anda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b159ed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected file paths:\n",
      "data/klasifikasi_kemiskinan.csv\n",
      "data/socio_economic_2021.csv\n",
      "data/pendidikan_provinsi_2023.csv\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data'\n",
    "Poverty_path = os.path.join(DATA_DIR, 'klasifikasi_kemiskinan.csv')\n",
    "Socio_path   = os.path.join(DATA_DIR, 'socio_economic_2021.csv')\n",
    "Edu_path     = os.path.join(DATA_DIR, 'pendidikan_provinsi_2023.csv')\n",
    "\n",
    "print('Expected file paths:')\n",
    "print(Poverty_path)\n",
    "print(Socio_path)\n",
    "print(Edu_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dafd355",
   "metadata": {},
   "source": [
    "## 3. Helper functions: safe CSV loader & column normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b07b6634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helpers ready\n"
     ]
    }
   ],
   "source": [
    "def safe_read_csv(path):\n",
    "    \"\"\"Try to read CSV with common encodings; return DataFrame or None.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f'File not found: {path}')\n",
    "        return None\n",
    "    encodings = ['utf-8', 'latin1', 'cp1252']\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            continue\n",
    "    # final try (pandas default) and let errors show\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f'Failed to read {path}: {e}')\n",
    "        return None\n",
    "\n",
    "\n",
    "def normalize_columns(df):\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns.str.strip()\n",
    "                  .str.lower()\n",
    "                  .str.replace(' ', '_')\n",
    "                  .str.replace('-','_')\n",
    "                  .str.replace('\\n','_')\n",
    "    )\n",
    "    return df\n",
    "\n",
    "print('helpers ready')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f83fc9",
   "metadata": {},
   "source": [
    "## 4. Load datasets (preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315ea847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: data/klasifikasi_kemiskinan.csv\n",
      "File not found: data/socio_economic_2021.csv\n",
      "File not found: data/pendidikan_provinsi_2023.csv\n",
      "poverty -> NOT LOADED\n",
      "socio -> NOT LOADED\n",
      "edu -> NOT LOADED\n"
     ]
    }
   ],
   "source": [
    "poverty = safe_read_csv(Poverty_path)\n",
    "socio = safe_read_csv(Socio_path)\n",
    "edu = safe_read_csv(Edu_path)\n",
    "\n",
    "# show what was loaded\n",
    "for name, df in [('poverty', poverty), ('socio', socio), ('edu', edu)]:\n",
    "    if df is None:\n",
    "        print(f\"{name} -> NOT LOADED\")\n",
    "    else:\n",
    "        print(f\"{name} -> loaded, shape: {df.shape}\")\n",
    "        display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93590cde",
   "metadata": {},
   "source": [
    "## 5. Inspect and normalize column names\n",
    "\n",
    "Kita normalisasi nama kolom supaya lebih mudah digabungkan/diolah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf518da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name, df in [('poverty', poverty), ('socio', socio), ('edu', edu)]:\n",
    "    if df is None:\n",
    "        continue\n",
    "    print('\\n---', var_name, 'columns ---')\n",
    "    df_norm = normalize_columns(df)\n",
    "    print(df_norm.columns.tolist()[:40])\n",
    "    # replace in-place reference\n",
    "    if var_name == 'poverty':\n",
    "        poverty = df_norm\n",
    "    elif var_name == 'socio':\n",
    "        socio = df_norm\n",
    "    elif var_name == 'edu':\n",
    "        edu = df_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6854d8a",
   "metadata": {},
   "source": [
    "## 6. Quick missing-value check and column suggestions\n",
    "\n",
    "Lihat ringkasan missing dan tipe data. Jika kolom target tidak ada namanya persis `tingkat_kemiskinan`, kita akan coba detect otomatis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a8f16fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poverty is None\n",
      "socio is None\n",
      "edu is None\n"
     ]
    }
   ],
   "source": [
    "def df_summary(df, name='df'):\n",
    "    if df is None:\n",
    "        print(f'{name} is None')\n",
    "        return\n",
    "    print(f\"{name}: shape={df.shape}\")\n",
    "    display(df.info())\n",
    "    display(pd.DataFrame({\n",
    "        'n_missing': df.isnull().sum(),\n",
    "        'pct_missing': df.isnull().mean()*100\n",
    "    }).sort_values('pct_missing', ascending=False).head(20))\n",
    "\n",
    "for name, df in [('poverty', poverty), ('socio', socio), ('edu', edu)]:\n",
    "    df_summary(df, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd21563",
   "metadata": {},
   "source": [
    "## 7. Find a suitable join key (provinsi / province / kode_prov)\n",
    "\n",
    "Kita cari kolom yang memungkinkan penggabungan ketiga dataset. Kalau dataset `poverty` berskala household, kita tetap menggabungkan fitur provinsi dari dataset pendidikan secara `map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af4af494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poverty join cols: []\n",
      "socio   join cols: []\n",
      "edu     join cols: []\n"
     ]
    }
   ],
   "source": [
    "def find_possible_join_cols(df):\n",
    "    candidates = ['provinsi','province','kode_prov','kode_provinsi','nama_provinsi','nama_prov','kabupaten','district']\n",
    "    cols = set(df.columns)\n",
    "    found = [c for c in candidates if c in cols]\n",
    "    # also check substring matching\n",
    "    for col in df.columns:\n",
    "        for cand in candidates:\n",
    "            if cand in col:\n",
    "                if col not in found:\n",
    "                    found.append(col)\n",
    "    return found\n",
    "\n",
    "print('poverty join cols:', find_possible_join_cols(poverty) if poverty is not None else [])\n",
    "print('socio   join cols:', find_possible_join_cols(socio) if socio is not None else [])\n",
    "print('edu     join cols:', find_possible_join_cols(edu) if edu is not None else [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcfbc5e",
   "metadata": {},
   "source": [
    "## 8. Merge datasets\n",
    "\n",
    "Strategi:\n",
    "- Jika semua dataset punya kolom `provinsi` (atau sejenis), kita merge pada tingkat provinsi.\n",
    "- Jika `poverty` adalah household-level (lebih granular), kita akan merge `socio` ke `poverty` berdasarkan kolom wilayah, dan gunakan data `edu` (provinsi) dengan mapping provinsi → indikator pendidikan.\n",
    "\n",
    "Catatan: sesuaikan nama kolom `join_key` jika notebook menemukan nama lain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36649522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join columns found: None\n",
      "No single join key found across all datasets. Merging socio+edu by provinsi if possible.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m     merged \u001b[38;5;241m=\u001b[39m poverty\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# keep a copy\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m df \u001b[38;5;241m=\u001b[39m merged\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResulting dataframe ready for preprocessing. Shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "# attempt to find join key automatically\n",
    "possible_keys = ['provinsi','province','kode_prov','nama_provinsi','nama_prov','prov']\n",
    "\n",
    "def find_join_for_all(dfs, keys):\n",
    "    for k in keys:\n",
    "        if all((df is not None) and any(k == c or k in c for c in df.columns) for df in dfs):\n",
    "            # return the actual column name from first df (match)\n",
    "            actual_cols = []\n",
    "            for df in dfs:\n",
    "                col = next((c for c in df.columns if (k == c or k in c)), None)\n",
    "                actual_cols.append(col)\n",
    "            return actual_cols\n",
    "    return None\n",
    "\n",
    "dfs = [poverty, socio, edu]\n",
    "join_cols = find_join_for_all(dfs, possible_keys)\n",
    "print('join columns found:', join_cols)\n",
    "\n",
    "# If we didn't find a common key, try merging socio+edu on provinsi and then attach socio to poverty where possible.\n",
    "merged = None\n",
    "if join_cols is not None:\n",
    "    # rename columns to a common name provinsi for merge\n",
    "    cols_map = {}\n",
    "    for df, actual_col in zip(dfs, join_cols):\n",
    "        if df is None or actual_col is None:\n",
    "            continue\n",
    "        df.columns = [c if c!=actual_col else 'provinsi' for c in df.columns]\n",
    "    try:\n",
    "        merged = poverty.merge(socio, on='provinsi', how='left')\n",
    "        merged = merged.merge(edu, on='provinsi', how='left')\n",
    "        print('merged shape:', merged.shape)\n",
    "    except Exception as e:\n",
    "        print('Merge error:', e)\n",
    "else:\n",
    "    print('No single join key found across all datasets. Merging socio+edu by provinsi if possible.')\n",
    "    # try merge socio + edu\n",
    "    socio_cols = find_possible_join_cols(socio) if socio is not None else []\n",
    "    edu_cols = find_possible_join_cols(edu) if edu is not None else []\n",
    "    common = set(socio_cols).intersection(set(edu_cols))\n",
    "    if common:\n",
    "        common_col = list(common)[0]\n",
    "        print('Merging socio and edu on', common_col)\n",
    "        merged_se = socio.merge(edu, left_on=common_col, right_on=common_col, how='left')\n",
    "        # then try attaching merged_se to poverty based on any shared column\n",
    "        shared = set(merged_se.columns).intersection(set(poverty.columns))\n",
    "        if shared:\n",
    "            shared_col = list(shared)[0]\n",
    "            print('Merging poverty with socio+edu on', shared_col)\n",
    "            merged = poverty.merge(merged_se, on=shared_col, how='left')\n",
    "            print('merged shape:', merged.shape)\n",
    "        else:\n",
    "            print('No shared column to merge poverty with socio+edu automatically. You might need to merge manually based on your data schema.')\n",
    "\n",
    "# if merge didn't run, keep merged = poverty for further steps (so notebook remains runnable)\n",
    "if merged is None and poverty is not None:\n",
    "    merged = poverty.copy()\n",
    "\n",
    "# keep a copy\n",
    "df = merged.copy()\n",
    "print('\\nResulting dataframe ready for preprocessing. Shape:', df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb18c2",
   "metadata": {},
   "source": [
    "## 9. Mendeteksi kolom target (`tingkat_kemiskinan`) secara otomatis\n",
    "\n",
    "Jika nama kolom target berbeda, notebook akan mencoba mendeteksi kolom yang mengandung kata `kemiskin` atau `poverty`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_target_column(df):\n",
    "    candidates = [c for c in df.columns if 'kemiskin' in c or 'poverty' in c or 'level' in c or 'status' in c]\n",
    "    if len(candidates)>0:\n",
    "        print('Possible target columns:', candidates)\n",
    "        return candidates[0]\n",
    "    # fallback: ask user to set target manually by editing the variable `TARGET_COL` below\n",
    "    return None\n",
    "\n",
    "TARGET_COL = detect_target_column(df)\n",
    "print('Detected target col:', TARGET_COL)\n",
    "\n",
    "if TARGET_COL is None:\n",
    "    print('\\n*** WARNING: target column not detected automatically. Please edit TARGET_COL to the correct column name from df.columns list below:')\n",
    "    print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f06b91d",
   "metadata": {},
   "source": [
    "## 10. Simple cleaning & feature selection\n",
    "\n",
    "- Memilih fitur numerik dan kategorikal\n",
    "- Imputasi sederhana\n",
    "- Membuat beberapa fitur baru contoh (jika tersedia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e76e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If TARGET_COL is None, set it manually here (edit as needed). E.g. TARGET_COL = 'tingkat_kemiskinan'\n",
    "# TARGET_COL = 'tingkat_kemiskinan'\n",
    "\n",
    "if TARGET_COL is None:\n",
    "    TARGET_COL = 'tingkat_kemiskinan'  # try default -- edit if necessary\n",
    "\n",
    "# Ensure TARGET_COL exists\n",
    "if TARGET_COL not in df.columns:\n",
    "    print('Target column not in df.columns; available cols:')\n",
    "    print(df.columns.tolist()[:80])\n",
    "    # we'll try to continue but user should edit the target if running locally\n",
    "\n",
    "# quick feature engineering examples (only if columns exist)\n",
    "# create per-capita features if penghasilan and anggota_keluarga exist\n",
    "if 'penghasilan' in df.columns and 'jumlah_angota_keluarga' in df.columns:\n",
    "    df['penghasilan_perkapita'] = df['penghasilan'] / (df['jumlah_angota_keluarga'].replace(0,1))\n",
    "\n",
    "# generic numeric / categorical split\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "# drop obvious identifier columns if present\n",
    "for c in ['id','id_rumah_tangga','no','index']:\n",
    "    if c in df.columns:\n",
    "        df = df.drop(columns=[c])\n",
    "\n",
    "print('Numerical cols (sample):', num_cols[:20])\n",
    "print('Categorical cols (sample):', cat_cols[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1082dd",
   "metadata": {},
   "source": [
    "## 11. Train/Test split\n",
    "\n",
    "Kita lakukan split stratified jika target terdeteksi. Jika target kategorikal, pastikan ia sudah berbentuk label (0/1/2...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TARGET_COL not in df.columns:\n",
    "    raise ValueError('Please set TARGET_COL to a valid column name in the dataframe before running modeling cells.')\n",
    "\n",
    "# drop rows with missing target\n",
    "df_model = df.copy()\n",
    "df_model = df_model[df_model[TARGET_COL].notnull()].reset_index(drop=True)\n",
    "\n",
    "# if target is string labels, convert to categorical codes\n",
    "if df_model[TARGET_COL].dtype == 'object' or str(df_model[TARGET_COL].dtype).startswith('category'):\n",
    "    df_model[TARGET_COL] = df_model[TARGET_COL].astype('category').cat.codes\n",
    "\n",
    "X = df_model.drop(columns=[TARGET_COL])\n",
    "y = df_model[TARGET_COL]\n",
    "\n",
    "# for reproducibility, select a manageable set of columns if too many\n",
    "MAX_FEATURES = 200\n",
    "if X.shape[1] > MAX_FEATURES:\n",
    "    print(f\"Warning: {X.shape[1]} features detected. Selecting top {MAX_FEATURES} numeric features by variance for baseline.\")\n",
    "    variances = X.var().sort_values(ascending=False)\n",
    "    keep = variances.index[:MAX_FEATURES].tolist()\n",
    "    # also keep categorical cols\n",
    "    cat_keep = [c for c in X.select_dtypes(include=['object','category']).columns]\n",
    "    keep = list(dict.fromkeys(keep + cat_keep))\n",
    "    X = X[keep]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "print('Train/Test shapes:', X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb906b67",
   "metadata": {},
   "source": [
    "## 12. Preprocessing pipelines (numerical & categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0c0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "print('Preprocessor ready')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2574a1f8",
   "metadata": {},
   "source": [
    "## 13. Modeling: baseline models (Logistic Regression, RandomForest, XGBoost if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef903d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n",
    "}\n",
    "if xgb is not None:\n",
    "    models['XGBoost'] = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[('preproc', preprocessor), ('model', model)])\n",
    "    print('\\nTraining', name)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    print('Classification report for', name)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    try:\n",
    "        if hasattr(pipe.named_steps['model'], 'predict_proba'):\n",
    "            y_proba = pipe.predict_proba(X_test)\n",
    "            # if multiclass, compute micro auc or skip\n",
    "            if y_proba.shape[1] == 2:\n",
    "                auc = roc_auc_score(y_test, y_proba[:,1])\n",
    "                print('ROC AUC:', auc)\n",
    "    except Exception:\n",
    "        pass\n",
    "    results[name] = pipe\n",
    "\n",
    "print('\\nModels trained: ', list(results.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eacbe32",
   "metadata": {},
   "source": [
    "## 14. Model comparison & selection\n",
    "\n",
    "Pilih model terbaik berdasarkan metrik (misalnya F1-score / ROC AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53862aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[('preproc', preprocessor), ('model', model)])\n",
    "    try:\n",
    "        sc = cross_val_score(pipe, X_train, y_train, cv=cv, scoring='f1_macro')\n",
    "        print(f\"{name} CV f1_macro: mean={sc.mean():.4f}, std={sc.std():.4f}\")\n",
    "    except Exception as e:\n",
    "        print('Cross-val error for', name, e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a41f0",
   "metadata": {},
   "source": [
    "## 15. Interpretability: Feature importance & SHAP (jika tersedia)\n",
    "\n",
    "Untuk model tree-based, kita bisa lihat feature importance. Untuk analisis mendalam, gunakan SHAP jika package terpasang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b05e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = list(results.keys())[0]\n",
    "print('Default best model choice (first trained):', best_model_name)\n",
    "# if XGBoost exists prefer that\n",
    "if 'XGBoost' in results:\n",
    "    best_model_name = 'XGBoost'\n",
    "elif 'RandomForest' in results:\n",
    "    best_model_name = 'RandomForest'\n",
    "\n",
    "best_pipe = results[best_model_name]\n",
    "\n",
    "# feature importance for tree models\n",
    "try:\n",
    "    model = best_pipe.named_steps['model']\n",
    "    # get feature names after preprocessing\n",
    "    # numeric feature names are numeric_features, categorical become onehot names\n",
    "    cat_ohe_cols = []\n",
    "    if len(categorical_features) > 0:\n",
    "        ohe = best_pipe.named_steps['preproc'].named_transformers_['cat'].named_steps['onehot']\n",
    "        cat_ohe_cols = ohe.get_feature_names_out(categorical_features).tolist()\n",
    "    feature_names = numeric_features + cat_ohe_cols\n",
    "\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        fi = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "        display(fi.head(30))\n",
    "    else:\n",
    "        print('Model has no attribute feature_importances_')\n",
    "except Exception as e:\n",
    "    print('Feature importance error:', e)\n",
    "\n",
    "# SHAP (optional)\n",
    "if shap is None:\n",
    "    print('\\nSHAP is not installed. To run SHAP, install it with `pip install shap`')\n",
    "else:\n",
    "    try:\n",
    "        # get transformed training data for SHAP\n",
    "        X_train_trans = best_pipe.named_steps['preproc'].transform(X_train)\n",
    "        explainer = shap.Explainer(best_pipe.named_steps['model'])\n",
    "        shap_values = explainer(X_train_trans)\n",
    "        shap.summary_plot(shap_values, X_train_trans, feature_names=feature_names)\n",
    "    except Exception as e:\n",
    "        print('SHAP error (common if model or data shape mismatches):', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e80a154",
   "metadata": {},
   "source": [
    "## 16. Save best model\n",
    "\n",
    "Simpan pipeline lengkap (preprocessing + model) untuk dipakai di production atau evaluasi lebih lanjut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_model_path = 'model_prediksi_kemiskinan_pipeline.joblib'\n",
    "joblib.dump(best_pipe, out_model_path)\n",
    "print('Saved pipeline to', out_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f061f6a",
   "metadata": {},
   "source": [
    "## 17. (Opsional) Membuat peta choropleth per provinsi\n",
    "\n",
    "Untuk membuat peta, Anda butuh file GeoJSON / shapefile batas provinsi Indonesia. Contoh kode di bawah mengasumsikan Anda memiliki `indonesia_prov.geojson` yang berisi properti `provinsi` atau `nama_prov` yang cocok dengan kolom `provinsi` di data Anda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import geopandas as gpd\n",
    "    import folium\n",
    "    has_geo = True\n",
    "except Exception:\n",
    "    has_geo = False\n",
    "\n",
    "if not has_geo:\n",
    "    print('geopandas or folium not installed. Skip mapping. To enable mapping, install geopandas and folium.')\n",
    "else:\n",
    "    # example: aggregate predictions per province and merge with geojson\n",
    "    # 1) read geojson\n",
    "    # geo = gpd.read_file('data/indonesia_prov.geojson')\n",
    "    # 2) create predictions per province (this part depends on your dataframe schema)\n",
    "    # agg = df_model.copy()\n",
    "    # agg['pred_proba'] = best_pipe.predict_proba(X)[:,1] if hasattr(best_pipe, 'predict_proba') else None\n",
    "    # agg_prov = agg.groupby('provinsi')['pred_proba'].mean().reset_index()\n",
    "    # geo = geo.merge(agg_prov, left_on='provinsi', right_on='provinsi')\n",
    "    # 3) create folium map\n",
    "    # m = folium.Map(location=[-2.5489, 118.0149], zoom_start=5)\n",
    "    # folium.Choropleth(geo_data=geo, data=geo, columns=['provinsi','pred_proba'], key_on='feature.properties.provinsi').add_to(m)\n",
    "    # display(m)\n",
    "    print('Mapping cell provided as an example. Please supply a geojson file and adapt the merging keys before running.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb7c69",
   "metadata": {},
   "source": [
    "## 18. Kesimpulan & Langkah Selanjutnya\n",
    "\n",
    "- Notebook ini memberi alur lengkap: load → merge → preprocessing → baseline models → evaluasi → interpretasi → simpan model.\n",
    "- Langkah selanjutnya yang direkomendasikan:\n",
    "  - Tuning hyperparameter (RandomizedSearchCV / Optuna)\n",
    "  - Menangani class imbalance (SMOTE / class weights)\n",
    "  - Validasi geografis: train di subset provinsi dan test di provinsi berbeda\n",
    "  - Deployment: buat API (FastAPI) atau dashboard (Streamlit)\n",
    "\n",
    "---\n",
    "\n",
    "Jika Anda mau, saya bisa:\n",
    "- Sesuaikan notebook ini ke nama file CSV Anda jika Anda beri tahu nama file (atau upload file CSV sekarang), atau\n",
    "- Jalankan notebook di environment ini jika Anda mengupload dataset ke percakapan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a3b6d0",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "*Notebook generated by ChatGPT — kalau ada bagian yang perlu disesuaikan (kolom target, nama kolom wilayah, dsb.) beri tahu saya dan saya akan perbaiki.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
